<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Intro to Large Language Models | </title> <meta name="author" content="Emmanuel Adeloju"> <meta name="description" content="Building a shared mental model of what Large Language Models actually are, and what they are not."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/EmmanuelAdeloju/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/EmmanuelAdeloju/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/EmmanuelAdeloju/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/EmmanuelAdeloju/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/EmmanuelAdeloju/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/intro-to-llm/"> <script src="/EmmanuelAdeloju/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/EmmanuelAdeloju/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/EmmanuelAdeloju/assets/js/distillpub/template.v2.js"></script> <script src="/EmmanuelAdeloju/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Intro to Large Language Models",
            "description": "Building a shared mental model of what Large Language Models actually are, and what they are not.",
            "published": "February 01, 2026",
            "authors": [
              
              {
                "author": "Emmanuel Adeloju",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Arizona State University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/EmmanuelAdeloju/"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/EmmanuelAdeloju/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/modules/">Course Modules </a> </li> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/EmmanuelAdeloju/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/EmmanuelAdeloju/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/EmmanuelAdeloju/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Intro to Large Language Models</h1> <p>Building a shared mental model of what Large Language Models actually are, and what they are not.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#why-this-module-matters">Why This Module Matters</a> </div> <div> <a href="#what-is-a-large-language-model-llm">What Is a Large Language Model (LLM)?</a> </div> <div> <a href="#llms-are-pattern-based-not-reasoning-agents">LLMs Are Pattern-Based, Not Reasoning Agents</a> </div> <div> <a href="#how-do-llms-learn-a-teacher-friendly-overview">How Do LLMs Learn? (A Teacher-Friendly Overview)</a> </div> <ul> <li> <a href="#training-at-a-high-level">Training at a High Level</a> </li> <li> <a href="#stage-1-data-collection-and-preprocessing">Stage 1: Data Collection and Preprocessing</a> </li> <li> <a href="#stage-2-pretraining-learning-language-patterns">Stage 2: Pretraining (Learning Language Patterns)</a> </li> <li> <a href="#stage-3-fine-tuning-specialization">Stage 3: Fine-Tuning (Specialization)</a> </li> </ul> <div> <a href="#prompting-and-prompt-engineering">Prompting and Prompt Engineering</a> </div> <ul> <li> <a href="#what-is-a-prompt">What Is a Prompt?</a> </li> <li> <a href="#what-is-prompt-engineering">What Is Prompt Engineering?</a> </li> </ul> <div> <a href="#why-llms-can-be-helpful-for-science-data-sensemaking">Why LLMs Can Be Helpful for Science Data Sensemaking</a> </div> <div> <a href="#major-risks-teachers-need-to-be-aware-of">Major Risks Teachers Need to Be Aware Of</a> </div> </nav> </d-contents> <h2 id="why-this-module-matters">Why This Module Matters</h2> <p>Before asking teachers to use AI for data analysis and sensemaking, it is important to slow down and build a <strong>shared mental model</strong> of what Large Language Models (LLMs) actually are—and what they are not.</p> <p>This matters because how we understand these tools directly shapes:</p> <ul> <li>how we use them in instruction,</li> <li>how much we trust them, and</li> <li>how we design prompts for them.</li> </ul> <p>Without a clear mental model, it is easy to overestimate what LLMs can do or to misuse them in ways that undermine student sensemaking rather than support it.</p> <hr> <h2 id="what-is-a-large-language-model-llm">What Is a Large Language Model (LLM)?</h2> <p>Large Language Models (LLMs) are a type of Artificial Intelligence (AI) designed to work with <strong>human language</strong>. They are trained on massive collections of text and learn <strong>statistical patterns in language</strong>, enabling them to generate new text that sounds coherent and contextually appropriate.</p> <p>In practical terms, LLMs can:</p> <ul> <li>respond to questions,</li> <li>summarize text,</li> <li>generate explanations,</li> <li>translate languages,</li> <li>write code, and</li> <li>support <em>language-based sensemaking</em> around data.</li> </ul> <p>Common examples include:</p> <ul> <li> <strong>ChatGPT</strong> (OpenAI)</li> <li> <strong>Claude</strong> (Anthropic)</li> <li> <strong>Gemini</strong> (Google)</li> <li> <strong>LLaMA</strong> (Meta)</li> </ul> <p>Most models teachers encounter are <strong>general-purpose models</strong>. This means they are designed to perform reasonably well across many tasks, rather than being experts in a single discipline such as statistics or data science.</p> <blockquote> <p><strong>Key takeaway:</strong><br> LLMs are <em>language models</em>, not data analysis engines or reasoning agents.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/EmmanuelAdeloju/assets/img/prompt-480.webp 480w,/EmmanuelAdeloju/assets/img/prompt-800.webp 800w,/EmmanuelAdeloju/assets/img/prompt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/EmmanuelAdeloju/assets/img/prompt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">A simplified view of how a prompt guides a Large Language Model to generate a response by predicting likely next words rather than reasoning from data.</figcaption> </figure> <p>Fig 1. A simplified view of how a prompt guides a Large Language Model to generate a response by predicting likely next words rather than reasoning from data</p> <hr> <h2 id="llms-are-pattern-based-not-reasoning-agents">LLMs Are Pattern-Based, Not Reasoning Agents</h2> <p>This is one of the most important ideas in this module.</p> <p>LLMs do not <em>understand</em> data, concepts, or scientific phenomena in the way humans do. Instead, they generate responses by predicting what words are most likely to come next based on patterns learned during training.</p> <p>When an LLM provides a convincing explanation of a graph or trend, it is <strong>not reasoning from first principles</strong>. It is drawing on patterns it has encountered in similar explanations across its training data.</p> <p>This is why LLM outputs can:</p> <ul> <li>sound confident even when incorrect,</li> <li>use appropriate scientific language without grounding claims in evidence, and</li> <li>miss contextual details that matter in classrooms.</li> </ul> <p>Understanding this limitation is essential for using LLMs productively in science education.</p> <hr> <h2 id="how-do-llms-learn-a-teacher-friendly-overview">How Do LLMs Learn? (A Teacher-Friendly Overview)</h2> <p>Teachers do not need all the technical details—but they <em>do</em> need intuition. Understanding how these models are trained helps us interact with them more critically and productively.</p> <h3 id="training-at-a-high-level">Training at a High Level</h3> <p>LLMs are trained in three main stages.</p> <h3 id="stage-1-data-collection-and-preprocessing">Stage 1: Data Collection and Preprocessing</h3> <p>The model is first exposed to enormous amounts of text from sources such as:</p> <ul> <li>websites,</li> <li>Wikipedia,</li> <li>books,</li> <li>articles, and</li> <li>other publicly available text.</li> </ul> <p>This data is cleaned and filtered to remove low-quality or duplicate content. While this process is extensive, it is imperfect—meaning biases and gaps can remain.</p> <p><strong>Educational implication:</strong><br> What the model “knows” depends entirely on what it has encountered in text form.</p> <h3 id="stage-2-pretraining-learning-language-patterns">Stage 2: Pretraining (Learning Language Patterns)</h3> <p>During pretraining, the model learns general patterns in language by predicting missing or next words across <strong>billions (or trillions) of words</strong>.</p> <p>For example, an earlier version of ChatGPT (GPT-3.5) was trained on approximately <strong>45 TB of compressed text before filtering</strong>, and about <strong>570 GB after filtering</strong> (Brown et al., 2020), representing hundreds of billions to trillions of words. Training on such broad and diverse text sources results in <strong>general-purpose models</strong> rather than deep expertise in any single discipline.</p> <p>For instance, if given the prompt:</p> <blockquote> <p><em>“When the temperature increases, the solubility of most solids in water…”</em></p> </blockquote> <p>The model learns that phrases like <em>“also increases”</em> are statistically likely to follow.</p> <p>This stage creates a model that can respond across many topics, but without disciplinary understanding.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/EmmanuelAdeloju/assets/img/llmprediction-480.webp 480w,/EmmanuelAdeloju/assets/img/llmprediction-800.webp 800w,/EmmanuelAdeloju/assets/img/llmprediction-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/EmmanuelAdeloju/assets/img/llmprediction.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Large Language Models generate responses by predicting the most probable next tokens based on patterns learned during training, not by reasoning from first principles.</figcaption> </figure> <h3 id="stage-3-fine-tuning-specialization">Stage 3: Fine-Tuning (Specialization)</h3> <p>After pretraining, models can be fine-tuned using smaller, more focused datasets or human feedback.</p> <p>Fine-tuning means adjusting a pretrained model so it performs better on specific types of tasks (e.g., education, coding, or scientific explanation). This process often relies on <strong>transfer learning</strong>—using general language knowledge as a foundation rather than learning from scratch.</p> <p><strong>Plain-language definition:</strong><br> <em>Transfer learning</em> means starting with a broadly trained model and refining it for a specific purpose.</p> <p><strong>Educational implication:</strong><br> Even “education-focused” AI tools still rely on general language patterns, not deep disciplinary understanding.</p> <hr> <h2 id="prompting-and-prompt-engineering">Prompting and Prompt Engineering</h2> <h3 id="what-is-a-prompt">What Is a Prompt?</h3> <p>A prompt is the input we give an LLM—usually text—that describes:</p> <ul> <li>the task,</li> <li>the context, and</li> <li>the expected type of response.</li> </ul> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Explain the trend shown in this graph to a 7th-grade student using evidence from the data.”</em></p> </blockquote> <h3 id="what-is-prompt-engineering">What Is Prompt Engineering?</h3> <p>Prompt engineering is the intentional design and refinement of prompts to:</p> <ul> <li>clarify the task,</li> <li>constrain the model’s response, and</li> <li>align outputs with instructional goals.</li> </ul> <p>In education, prompt engineering is less about “hacking” the model and more about <strong>pedagogical precision</strong>.</p> <p>I think of prompt engineering as <strong>lesson design for an AI collaborator</strong>.</p> <p>We will discuss Prompt Engineering more in Module 2.</p> <hr> <h2 id="why-llms-can-be-helpful-for-science-data-sensemaking">Why LLMs Can Be Helpful for Science Data Sensemaking</h2> <p>Despite their limitations, LLMs can be powerful sensemaking supports when used carefully.</p> <p>They are especially useful for:</p> <ul> <li>translating data patterns into explanatory language,</li> <li>generating multiple interpretations of the same dataset,</li> <li>supporting claim–evidence–reasoning (CER),</li> <li>helping teachers anticipate student misconceptions.</li> </ul> <p>What they cannot do reliably is:</p> <ul> <li>validate conclusions,</li> <li>detect subtle data errors, or</li> <li>replace student reasoning or disciplinary judgment.</li> </ul> <hr> <h2 id="major-risks-teachers-need-to-be-aware-of">Major Risks Teachers Need to Be Aware Of</h2> <p>To use LLMs responsibly, we need to be explicit about the risks.</p> <h3 id="attributing-human-like-reasoning">Attributing Human-Like Reasoning</h3> <p>Fluent language can easily be mistaken for genuine understanding. This may lead to overconfidence in explanations that are not grounded in the data.</p> <h3 id="uncritical-trust-in-outputs">Uncritical Trust in Outputs</h3> <p>Repeated “good enough” responses can create a false sense of reliability. Without verification, incorrect interpretations may go unnoticed.</p> <h3 id="invisible-influence-on-thinking">Invisible Influence on Thinking</h3> <p>LLMs do not just answer questions—they <strong>frame problems</strong>. Their suggestions can subtly shape how data is interpreted or narrow the range of explanations considered.</p> <h3 id="displacement-of-collaborative-sensemaking">Displacement of Collaborative Sensemaking</h3> <p>Over-reliance on AI can reduce opportunities for productive struggle, discussion, and collaborative reasoning, all of which are essential in science learning.</p> <h3 id="erosion-of-analytical-judgment">Erosion of Analytical Judgment</h3> <p>When teachers or students defer too often to AI-generated interpretations, their ability to independently evaluate evidence and detect errors may weaken over time.</p> <hr> <h2 id="closing-reflection">Closing Reflection</h2> <p>The goal of this module is not to convince you that LLMs are good or bad. Rather, it is to help you see these tools clearly—as powerful language-based systems that can support science data sensemaking <strong>only when humans remain firmly in the loop</strong>.</p> <p>This understanding sets the foundation for the next module, where we move from <em>what LLMs are</em> to <em>how we design prompts</em> that support meaningful data analysis in classrooms.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/EmmanuelAdeloju/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/EmmanuelAdeloju/blog/2025/plotly/">a post with plotly.js</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/EmmanuelAdeloju/blog/2024/photo-gallery/">a post with image galleries</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/EmmanuelAdeloju/blog/2024/tabs/">a post with tabs</a> </li> <br> <br> <div id="giscus_thread"> <script defer src="/EmmanuelAdeloju/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Emmanuel Adeloju. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/EmmanuelAdeloju/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/EmmanuelAdeloju/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/EmmanuelAdeloju/assets/js/mermaid-setup.js?v=38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/EmmanuelAdeloju/assets/js/chartjs-setup.js?v=183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/EmmanuelAdeloju/assets/js/echarts-setup.js?v=738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/EmmanuelAdeloju/assets/js/vega-setup.js?v=7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/EmmanuelAdeloju/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/EmmanuelAdeloju/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/EmmanuelAdeloju/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/EmmanuelAdeloju/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/EmmanuelAdeloju/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/EmmanuelAdeloju/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/EmmanuelAdeloju/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/EmmanuelAdeloju/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/EmmanuelAdeloju/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/EmmanuelAdeloju/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/EmmanuelAdeloju/assets/js/search-data.js"></script> <script src="/EmmanuelAdeloju/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>