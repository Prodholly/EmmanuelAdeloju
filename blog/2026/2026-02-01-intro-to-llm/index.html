<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="why-this-module-matters">Why This Module Matters</h2> <p>Before asking teachers to use AI for data analysis and sensemaking, it is important to slow down and build a <strong>shared mental model</strong> of what Large Language Models (LLMs) actually are—and what they are not.</p> <p>This matters because how we understand these tools directly shapes:</p> <ul> <li>how we use them in instruction,</li> <li>how much we trust them, and</li> <li>how we design prompts for them.</li> </ul> <p>Without a clear mental model, it is easy to overestimate what LLMs can do or to misuse them in ways that undermine student sensemaking rather than support it.</p> <hr> <h2 id="what-is-a-large-language-model-llm">What Is a Large Language Model (LLM)?</h2> <p>Large Language Models (LLMs) are a type of Artificial Intelligence (AI) designed to work with <strong>human language</strong>. They are trained on massive collections of text and learn <strong>statistical patterns in language</strong>, enabling them to generate new text that sounds coherent and contextually appropriate.</p> <p>In practical terms, LLMs can:</p> <ul> <li>respond to questions,</li> <li>summarize text,</li> <li>generate explanations,</li> <li>translate languages,</li> <li>write code, and</li> <li>support <em>language-based sensemaking</em> around data.</li> </ul> <p>Common examples include:</p> <ul> <li> <strong>ChatGPT</strong> (OpenAI)</li> <li> <strong>Claude</strong> (Anthropic)</li> <li> <strong>Gemini</strong> (Google)</li> <li> <strong>LLaMA</strong> (Meta)</li> </ul> <p>Most models teachers encounter are <strong>general-purpose models</strong>. This means they are designed to perform reasonably well across many tasks, rather than being experts in a single discipline such as statistics or data science.</p> <blockquote> <p><strong>Key takeaway:</strong><br> LLMs are <em>language models</em>, not data analysis engines or reasoning agents.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/EmmanuelAdeloju/assets/img/prompt-480.webp 480w,/EmmanuelAdeloju/assets/img/prompt-800.webp 800w,/EmmanuelAdeloju/assets/img/prompt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/EmmanuelAdeloju/assets/img/prompt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">A simplified view of how a prompt guides a Large Language Model to generate a response by predicting likely next words rather than reasoning from data.</figcaption> </figure> <p>Fig 1. A simplified view of how a prompt guides a Large Language Model to generate a response by predicting likely next words rather than reasoning from data</p> <hr> <h2 id="llms-are-pattern-based-not-reasoning-agents">LLMs Are Pattern-Based, Not Reasoning Agents</h2> <p>This is one of the most important ideas in this module.</p> <p>LLMs do not <em>understand</em> data, concepts, or scientific phenomena in the way humans do. Instead, they generate responses by predicting what words are most likely to come next based on patterns learned during training.</p> <p>When an LLM provides a convincing explanation of a graph or trend, it is <strong>not reasoning from first principles</strong>. It is drawing on patterns it has encountered in similar explanations across its training data.</p> <p>This is why LLM outputs can:</p> <ul> <li>sound confident even when incorrect,</li> <li>use appropriate scientific language without grounding claims in evidence, and</li> <li>miss contextual details that matter in classrooms.</li> </ul> <p>Understanding this limitation is essential for using LLMs productively in science education.</p> <hr> <h2 id="how-do-llms-learn-a-teacher-friendly-overview">How Do LLMs Learn? (A Teacher-Friendly Overview)</h2> <p>Teachers do not need all the technical details—but they <em>do</em> need intuition. Understanding how these models are trained helps us interact with them more critically and productively.</p> <h3 id="training-at-a-high-level">Training at a High Level</h3> <p>LLMs are trained in three main stages.</p> <h3 id="stage-1-data-collection-and-preprocessing">Stage 1: Data Collection and Preprocessing</h3> <p>The model is first exposed to enormous amounts of text from sources such as:</p> <ul> <li>websites,</li> <li>Wikipedia,</li> <li>books,</li> <li>articles, and</li> <li>other publicly available text.</li> </ul> <p>This data is cleaned and filtered to remove low-quality or duplicate content. While this process is extensive, it is imperfect—meaning biases and gaps can remain.</p> <p><strong>Educational implication:</strong><br> What the model “knows” depends entirely on what it has encountered in text form.</p> <h3 id="stage-2-pretraining-learning-language-patterns">Stage 2: Pretraining (Learning Language Patterns)</h3> <p>During pretraining, the model learns general patterns in language by predicting missing or next words across <strong>billions (or trillions) of words</strong>.</p> <p>For example, an earlier version of ChatGPT (GPT-3.5) was trained on approximately <strong>45 TB of compressed text before filtering</strong>, and about <strong>570 GB after filtering</strong> (Brown et al., 2020), representing hundreds of billions to trillions of words. Training on such broad and diverse text sources results in <strong>general-purpose models</strong> rather than deep expertise in any single discipline.</p> <p>For instance, if given the prompt:</p> <blockquote> <p><em>“When the temperature increases, the solubility of most solids in water…”</em></p> </blockquote> <p>The model learns that phrases like <em>“also increases”</em> are statistically likely to follow.</p> <p>This stage creates a model that can respond across many topics, but without disciplinary understanding.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/EmmanuelAdeloju/assets/img/llmprediction-480.webp 480w,/EmmanuelAdeloju/assets/img/llmprediction-800.webp 800w,/EmmanuelAdeloju/assets/img/llmprediction-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/EmmanuelAdeloju/assets/img/llmprediction.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Large Language Models generate responses by predicting the most probable next tokens based on patterns learned during training, not by reasoning from first principles.</figcaption> </figure> <h3 id="stage-3-fine-tuning-specialization">Stage 3: Fine-Tuning (Specialization)</h3> <p>After pretraining, models can be fine-tuned using smaller, more focused datasets or human feedback.</p> <p>Fine-tuning means adjusting a pretrained model so it performs better on specific types of tasks (e.g., education, coding, or scientific explanation). This process often relies on <strong>transfer learning</strong>—using general language knowledge as a foundation rather than learning from scratch.</p> <p><strong>Plain-language definition:</strong><br> <em>Transfer learning</em> means starting with a broadly trained model and refining it for a specific purpose.</p> <p><strong>Educational implication:</strong><br> Even “education-focused” AI tools still rely on general language patterns, not deep disciplinary understanding.</p> <hr> <h2 id="prompting-and-prompt-engineering">Prompting and Prompt Engineering</h2> <h3 id="what-is-a-prompt">What Is a Prompt?</h3> <p>A prompt is the input we give an LLM—usually text—that describes:</p> <ul> <li>the task,</li> <li>the context, and</li> <li>the expected type of response.</li> </ul> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Explain the trend shown in this graph to a 7th-grade student using evidence from the data.”</em></p> </blockquote> <h3 id="what-is-prompt-engineering">What Is Prompt Engineering?</h3> <p>Prompt engineering is the intentional design and refinement of prompts to:</p> <ul> <li>clarify the task,</li> <li>constrain the model’s response, and</li> <li>align outputs with instructional goals.</li> </ul> <p>In education, prompt engineering is less about “hacking” the model and more about <strong>pedagogical precision</strong>.</p> <p>I think of prompt engineering as <strong>lesson design for an AI collaborator</strong>.</p> <p>We will discuss Prompt Engineering more in Module 2.</p> <hr> <h2 id="why-llms-can-be-helpful-for-science-data-sensemaking">Why LLMs Can Be Helpful for Science Data Sensemaking</h2> <p>Despite their limitations, LLMs can be powerful sensemaking supports when used carefully.</p> <p>They are especially useful for:</p> <ul> <li>translating data patterns into explanatory language,</li> <li>generating multiple interpretations of the same dataset,</li> <li>supporting claim–evidence–reasoning (CER),</li> <li>helping teachers anticipate student misconceptions.</li> </ul> <p>What they cannot do reliably is:</p> <ul> <li>validate conclusions,</li> <li>detect subtle data errors, or</li> <li>replace student reasoning or disciplinary judgment.</li> </ul> <hr> <h2 id="major-risks-teachers-need-to-be-aware-of">Major Risks Teachers Need to Be Aware Of</h2> <p>To use LLMs responsibly, we need to be explicit about the risks.</p> <h3 id="attributing-human-like-reasoning">Attributing Human-Like Reasoning</h3> <p>Fluent language can easily be mistaken for genuine understanding. This may lead to overconfidence in explanations that are not grounded in the data.</p> <h3 id="uncritical-trust-in-outputs">Uncritical Trust in Outputs</h3> <p>Repeated “good enough” responses can create a false sense of reliability. Without verification, incorrect interpretations may go unnoticed.</p> <h3 id="invisible-influence-on-thinking">Invisible Influence on Thinking</h3> <p>LLMs do not just answer questions—they <strong>frame problems</strong>. Their suggestions can subtly shape how data is interpreted or narrow the range of explanations considered.</p> <h3 id="displacement-of-collaborative-sensemaking">Displacement of Collaborative Sensemaking</h3> <p>Over-reliance on AI can reduce opportunities for productive struggle, discussion, and collaborative reasoning, all of which are essential in science learning.</p> <h3 id="erosion-of-analytical-judgment">Erosion of Analytical Judgment</h3> <p>When teachers or students defer too often to AI-generated interpretations, their ability to independently evaluate evidence and detect errors may weaken over time.</p> <hr> <h2 id="closing-reflection">Closing Reflection</h2> <p>The goal of this module is not to convince you that LLMs are good or bad. Rather, it is to help you see these tools clearly—as powerful language-based systems that can support science data sensemaking <strong>only when humans remain firmly in the loop</strong>.</p> <p>This understanding sets the foundation for the next module, where we move from <em>what LLMs are</em> to <em>how we design prompts</em> that support meaningful data analysis in classrooms.</p> </body></html>