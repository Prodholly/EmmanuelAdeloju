<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://prodholly.github.io/EmmanuelAdeloju/feed.xml" rel="self" type="application/atom+xml"/><link href="https://prodholly.github.io/EmmanuelAdeloju/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-15T10:43:26+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/feed.xml</id><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Communicate and Propose Action</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/communicate-action/" rel="alternate" type="text/html" title="Communicate and Propose Action"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/communicate-action</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/communicate-action/"><![CDATA[<h2 id="context-using-electrical-fire-data-to-inform-decisions">Context: Using Electrical Fire Data to Inform Decisions</h2> <p>Throughout this module, we continue working with our running example of investigating electrical fire incidents, now focusing on how to communicate findings and propose evidence-based actions.</p> <hr/> <h2 id="why-this-phase-matters">Why This Phase Matters</h2> <p>Data investigations do not end with models or visualizations. They end when evidence is translated into understanding and action. In this phase, you connect results back to the real-world problem, craft a clear data story, and propose actions that are supported by evidence.</p> <p>Communication is not an afterthought, it is part of the analysis. How results are framed, explained, and delivered directly affects whether insights are understood, trusted, and used.</p> <hr/> <h2 id="from-results-to-meaning">From Results to Meaning</h2> <p>At this point in the investigation, you have:</p> <ul> <li>Cleaned and explored electrical fire data</li> <li>Built and evaluated models</li> <li>Identified patterns, relationships, and uncertainty</li> </ul> <p>Now, you ask:</p> <ul> <li>What do these results mean in context?</li> <li>What claims can be supported by evidence?</li> <li>What actions are reasonable given the limits of the data?</li> </ul> <hr/> <h2 id="step-by-step-communication-and-action-workflow">Step-by-Step Communication and Action Workflow</h2> <p>The following six-step workflow guides the process of communicating findings and proposing evidence-based actions.</p> <hr/> <h2 id="step-1-reconnect-findings-to-the-original-problem">Step 1: Reconnect Findings to the Original Problem</h2> <p>Begin by returning to the broader issue that motivated the investigation.</p> <p><strong>Electrical fire example:</strong></p> <p><strong>Broader issue:</strong> Community safety and infrastructure reliability</p> <p><strong>Investigative question:</strong> How does building age relate to electrical fire incidents?</p> <p>Here, explicitly connect models and visualizations back to the problem they were meant to address.</p> <p><strong>Prompting technique:</strong> Zero-shot prompting</p> <blockquote> <p><em>“Summarize how the results of this analysis address the original question about electrical fire risk.”</em></p> </blockquote> <hr/> <h2 id="step-2-make-evidence-based-claims">Step 2: Make Evidence-Based Claims</h2> <p>Translate results into claims that are directly supported by data. Each claim is paired with specific evidence.</p> <p><strong>Example claims:</strong></p> <ul> <li>“Electrical fires occur more frequently in buildings older than 30 years.”</li> <li>“Copper wiring appears in most incidents, but this may reflect its prevalence rather than increased risk.”</li> </ul> <p>Avoid overstating conclusions and clearly acknowledge uncertainty.</p> <p><strong>Prompting technique:</strong> Chain-of-thought prompting</p> <blockquote> <p><em>“Step by step, distinguish which conclusions are strongly supported by the data and which are tentative.”</em></p> </blockquote> <hr/> <h2 id="step-3-craft-a-data-story">Step 3: Craft a Data Story</h2> <p>Rather than presenting isolated charts, organize findings into a coherent narrative:</p> <ul> <li>The problem and why it matters</li> <li>What the data shows</li> <li>What the models suggest</li> <li>What remains uncertain</li> </ul> <p>A good data story helps others follow the reasoning without needing to inspect every technical detail.</p> <p><strong>Prompting technique:</strong> Few-shot prompting</p> <blockquote> <p><em>“Here are two examples of data stories used in public safety reports.</em><br/> <em>Based on these, help structure a clear narrative for the electrical fire findings.”</em></p> </blockquote> <hr/> <h2 id="step-4-propose-actions-supported-by-evidence">Step 4: Propose Actions Supported by Evidence</h2> <p>Propose actions that are:</p> <ul> <li>Grounded in the findings</li> <li>Appropriate to the data’s scope</li> <li>Explicit about limitations</li> </ul> <p><strong>Electrical fire examples:</strong></p> <ul> <li>Prioritize electrical inspections in older buildings</li> <li>Target public awareness campaigns in neighborhoods with aging infrastructure</li> <li>Collect additional data on wiring condition, not just age</li> </ul> <p>Actions are framed as recommendations, not prescriptions.</p> <p><strong>Prompting technique:</strong> Prompt chaining</p> <p><strong>Prompt 1:</strong></p> <blockquote> <p><em>“Based on these findings, list reasonable actions supported by the data.”</em></p> </blockquote> <p><strong>Prompt 2:</strong></p> <blockquote> <p><em>“For each action, explain what evidence supports it and what uncertainties remain.”</em></p> </blockquote> <hr/> <h2 id="step-5-tailor-communication-to-the-audience">Step 5: Tailor Communication to the Audience</h2> <p>Different stakeholders require different levels of detail and language.</p> <p><strong>Possible audiences include:</strong></p> <ul> <li>Community members</li> <li>School administrators</li> <li>City officials</li> <li>Fire safety professionals</li> </ul> <p>Adapt:</p> <ul> <li>Vocabulary and tone</li> <li>Visual complexity</li> <li>Format (report, infographic, slide deck, one-page guide)</li> </ul> <p><strong>Prompting technique:</strong> Self-consistency</p> <blockquote> <p><em>“Explain these findings once for a technical audience and once for a general audience.</em><br/> <em>Check that both versions remain consistent with the evidence.”</em></p> </blockquote> <hr/> <h2 id="step-6-reflect-revise-and-extend">Step 6: Reflect, Revise, and Extend</h2> <p>Often, proposing actions reveals new questions:</p> <ul> <li>What data is missing?</li> <li>What assumptions need testing?</li> <li>What should be investigated next?</li> </ul> <p>This phase frequently leads to:</p> <ul> <li>Revisiting earlier data</li> <li>Collecting additional information</li> <li>Framing a new investigative question</li> </ul> <p>This is not failure, it is how data investigations continue.</p> <hr/> <h2 id="key-takeaway">Key Takeaway</h2> <p>Communicating data is not about presenting results, it is about <strong>enabling informed decisions</strong>. In the context of electrical fire investigations, this means telling a clear, honest data story that connects evidence to action while respecting uncertainty.</p> <p><strong>LLMs can help with:</strong></p> <ul> <li>Structuring narratives</li> <li>Translating findings across audiences</li> <li>Stress-testing claims and recommendations</li> </ul> <p>But responsibility for interpretation, ethics, and action always remains with the investigator.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="data communication"/><category term="science communication"/><category term="evidence-based decisions"/><category term="data storytelling"/><category term="science education"/><summary type="html"><![CDATA[Translating data investigation results into clear narratives and evidence-based recommendations that inform real-world decisions.]]></summary></entry><entry><title type="html">Consider Data</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/consider-data/" rel="alternate" type="text/html" title="Consider Data"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/consider-data</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/consider-data/"><![CDATA[<h2 id="why-this-module-matters">Why This Module Matters</h2> <p>Once a problem is framed and a research question is defined, the next step is to ask:</p> <p><strong>What data can actually help us answer this question, and can we trust it?</strong></p> <p>In real data investigations, data does not magically appear clean, complete, or perfectly aligned with our goals. According to Lee et al. (2022), considering and gathering data involves understanding what data exists, what data is missing, how it was collected, and whose interests it represents.</p> <p>This module is where students begin to act like <strong>data investigators, not just data users</strong>.</p> <hr/> <h2 id="learning-goals">Learning Goals</h2> <p>By the end of this module, teachers and students will be able to:</p> <ul> <li>Identify what data is needed to answer a research question</li> <li>Evaluate data sources for relevance, quality, and bias</li> <li>Understand how data attributes are defined and measured</li> <li>Make informed decisions about storing and organizing datasets</li> <li>Use LLMs strategically to support data evaluation and planning</li> </ul> <hr/> <h2 id="running-example-continued-context">Running Example (Continued Context)</h2> <p><strong>Framed Research Question:</strong></p> <blockquote> <p><em>What types of electrical wire materials are most commonly associated with residential electrical fire incidents?</em></p> </blockquote> <p>All examples in this module connect back to this question.</p> <hr/> <h2 id="step-1-identify-the-data-needed">Step 1: Identify the Data Needed</h2> <p>Before opening any dataset, articulate what data would be useful.</p> <p><strong>Key questions:</strong></p> <ul> <li>What variables would help answer the research question?</li> <li>What measurements matter?</li> <li>What units should those measurements be in?</li> </ul> <h3 id="example-variables-electrical-fire-case">Example Variables (Electrical Fire Case)</h3> <ul> <li>Wire material (e.g., aluminum, copper)</li> <li>Year of installation</li> <li>Location of fire</li> <li>Severity of damage</li> </ul> <h3 id="llm-support">LLM Support</h3> <p><strong>Prompting technique:</strong> Zero-shot prompting (best for Understand level tasks)</p> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Based on this [insert research question here], list the types of data variables that would be useful for investigating it.”</em></p> </blockquote> <p>The LLM helps generate ideas, but teachers and students must decide what is actually feasible and relevant.</p> <hr/> <h2 id="step-2-assess-data-provenance">Step 2: Assess Data Provenance</h2> <p><strong>Data provenance</strong> refers to where the data originates, who collected it, and why it exists. The data provenance can greatly influence the kind of insight generated. Also, there might be bias or some factors embedded in the data that may make it unfit for your task.</p> <p>Students should investigate:</p> <ul> <li>Who collected the data?</li> <li>For what purpose?</li> <li>Was it collected for safety, research, policy, or another reason?</li> </ul> <h3 id="example">Example</h3> <p>Fire incident data might come from:</p> <ul> <li>A city fire department</li> <li>A state safety agency</li> <li>Insurance records</li> </ul> <p>Each source implies different priorities and limitations.</p> <h3 id="llm-support-1">LLM Support</h3> <p><strong>Prompting technique:</strong> Chain-of-Thought Prompting (Best for: Analyze / Evaluate tasks)</p> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“This dataset comes from a municipal fire department.</em><br/> <em>Step by step, analyze the strengths and limitations of this data source for investigating electrical fire causes.</em><br/> <em>Consider how the data was collected, what variables are included or missing, and any potential sources of bias.”</em></p> </blockquote> <p>Here, the model is asked to evaluate the data source rather than describe it.</p> <hr/> <h2 id="step-3-read-and-interpret-metadata">Step 3: Read and Interpret Metadata</h2> <p>Metadata includes information about:</p> <ul> <li>Column names</li> <li>Units of measurement</li> <li>Data descriptions</li> <li>Collection dates</li> </ul> <p>Many people often skip this step, but <strong>it’s where misunderstandings begin</strong>.</p> <h3 id="example-1">Example</h3> <p>A column labeled <code class="language-plaintext highlighter-rouge">WIRE_TYPE</code> might:</p> <ul> <li>Use codes instead of names</li> <li>Combine multiple materials into one category</li> <li>Be missing values for older buildings</li> </ul> <h3 id="llm-support-2">LLM Support</h3> <p><strong>Prompting technique:</strong> Few-shot prompting (best for Understand / Apply tasks)</p> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Here is an example of a well-described dataset column and explanation.</em><br/> <em>Now help interpret these column names and suggest questions a student should ask about them.”</em></p> </blockquote> <p>Few-shot prompting helps the LLM model mirror good interpretive habits.</p> <hr/> <h2 id="step-4-consider-bias-and-ethical-implications">Step 4: Consider Bias and Ethical Implications</h2> <p>Not all data represents everyone equally.</p> <p>Students should ask:</p> <ul> <li>Who is included in this data?</li> <li>Who might be missing?</li> <li>Does the data reflect reporting practices rather than actual incidence?</li> </ul> <h3 id="example-2">Example</h3> <p>Electrical fires may be underreported in:</p> <ul> <li>Older buildings</li> <li>Low-income neighborhoods</li> <li>Rural areas</li> </ul> <h3 id="llm-support-3">LLM Support</h3> <p><strong>Prompting technique:</strong> Self-consistency prompting (best for Analyze / Evaluate tasks)</p> <p><strong>Example prompt:</strong></p> <blockquote> <p><strong>Take note to use this same prompt 3 different times and compare answers.</strong></p> <p><em>“Suggest three different ways this dataset [attach dataset] might be biased.</em><br/> <em>Then explain which bias is most concerning for this investigation and why.”</em></p> </blockquote> <p>This encourages multiple perspectives before settling on one interpretation.</p> <hr/> <h2 id="step-5-decide-whether-additional-data-is-needed">Step 5: Decide Whether Additional Data Is Needed</h2> <p>Sometimes the available data is not enough.</p> <p>Students must decide:</p> <ul> <li>Do we need more samples?</li> <li>Do we need different attributes?</li> <li>Do we need data from another source?</li> </ul> <h3 id="example-3">Example</h3> <p>The dataset might list fire incidents but not wire material. That signals a need for:</p> <ul> <li>Building inspection records</li> <li>Construction databases</li> </ul> <h3 id="llm-support-4">LLM Support</h3> <p><strong>Prompting technique:</strong> Self-consistency prompting (best for Analyze / Create tasks)</p> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Given this dataset’s limitations, propose three ways to strengthen the investigation using additional data.”</em></p> </blockquote> <hr/> <h2 id="step-6-choose-storage-format-and-organize-data">Step 6: Choose Storage Format and Organize Data</h2> <p>Once data is selected, students should standardize how it is stored.</p> <p><strong>Best practices:</strong></p> <ul> <li>Use a consistent format (Excel recommended for K–12)</li> <li>Clear file naming (e.g., <code class="language-plaintext highlighter-rouge">FireData_2024_Cleaned.xlsx</code>)</li> <li>Document any changes made to the data</li> </ul> <h3 id="llm-support-5">LLM Support</h3> <p><strong>Prompting technique:</strong> Zero-shot prompting (best for Apply tasks)</p> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Suggest a clear file naming convention for this dataset and explain why it supports good data practice.”</em></p> </blockquote> <hr/> <h2 id="step-7-optional-merge-or-join-datasets">Step 7: (Optional) Merge or Join Datasets</h2> <p>If multiple datasets are relevant, students may explore joining them using:</p> <ul> <li>Date</li> <li>Location</li> <li>Building ID</li> </ul> <p>This step reinforces that data analysis decisions affect results.</p> <h3 id="llm-support-6">LLM Support</h3> <p><strong>Prompting technique:</strong> Prompt chaining (best for Apply → Analyze tasks)</p> <p><strong>Example chain:</strong></p> <p><strong>Prompt 1:</strong></p> <blockquote> <p><em>“What variables could be used to join these datasets?”</em></p> </blockquote> <p><strong>Prompt 2:</strong></p> <blockquote> <p><em>“What problems might arise when joining on this variable?”</em></p> </blockquote> <hr/> <h2 id="closing-the-module">Closing the Module</h2> <p>In this module, students learn that data is not neutral, complete, or self-explanatory. Considering data carefully—its source, structure, limitations, and ethics—is essential for meaningful science investigations.</p> <p>LLMs play a valuable role here as thinking partners, helping students ask better questions about data rather than rushing into analysis.</p> <p>In the next module, we move into exploring and visualizing data, where patterns, variability, and evidence begin to take shape.</p> <hr/> <p><strong>Reference:</strong></p> <p>Lee, V. R., Wilkerson, M. H., &amp; Lanouette, K. (2022). A call for a reimagined science education data literacy. <em>Journal of Research in Science Teaching</em>, 59(6), 1019-1049.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="data investigation"/><category term="data quality"/><category term="metadata"/><category term="data ethics"/><category term="science education"/><summary type="html"><![CDATA[Learning to evaluate data sources, understand metadata, and make informed decisions about data quality and ethical implications in science investigations.]]></summary></entry><entry><title type="html">Consider Models</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/consider-models/" rel="alternate" type="text/html" title="Consider Models"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/consider-models</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/consider-models/"><![CDATA[<h2 id="context-explaining-and-predicting-electrical-fire-incidents">Context: Explaining and Predicting Electrical Fire Incidents</h2> <p>Throughout this module, we continue working with our running example of investigating electrical fire incidents, now focusing on how models help us move from patterns to evidence-based claims.</p> <hr/> <h2 id="why-models-matter">Why Models Matter</h2> <p>After exploring and visualizing data, the next step is to select models that help answer your investigative question.</p> <p>A <strong>model</strong> is not just an equation, it is a simplified representation of reality that helps you explain patterns, test claims, or make predictions while acknowledging variability and uncertainty.</p> <p>In the context of electrical fire investigations, models help move from “what we see” to “what evidence supports a claim.”</p> <hr/> <h2 id="what-counts-as-a-model">What Counts as a Model?</h2> <p>In this module, models include:</p> <ul> <li>Statistical summaries (means, rates, proportions)</li> <li>Visual models (trend lines, distributions)</li> <li>Relationship models (correlation, regression)</li> <li>Predictive or classification models (when appropriate)</li> </ul> <p>Not every model you try will be useful. A key part of this phase is choosing which models to keep, and which to discard.</p> <hr/> <h2 id="modeling-is-iterative">Modeling Is Iterative</h2> <p>Modeling rarely happens in a straight line. Often, when testing a model, you realize that:</p> <ul> <li>Variables need further transformation</li> <li>Data needs to be regrouped or filtered</li> <li>Additional data may be required</li> </ul> <p>Going back to earlier phases is expected—and productive.</p> <hr/> <h2 id="step-by-step-modeling-workflow">Step-by-Step Modeling Workflow</h2> <p>The following six-step workflow guides the process of selecting, evaluating, and iterating on models for data investigations.</p> <hr/> <h2 id="step-1-clarify-the-modeling-goal">Step 1: Clarify the Modeling Goal</h2> <p>Begin by restating your investigative question and asking what kind of evidence would meaningfully address it.</p> <p><strong>Electrical fire examples:</strong></p> <ul> <li>Do older buildings experience more electrical fires? → <strong>Inference</strong></li> <li>Can we estimate fire risk based on building age and wiring type? → <strong>Prediction</strong></li> <li>Which factors appear most strongly associated with fire incidents? → <strong>Explanation</strong></li> </ul> <p>This distinction matters because different goals require different models.</p> <p><strong>Prompting technique:</strong> Zero-shot prompting</p> <blockquote> <p><em>“Given this investigative question about electrical fires, is the goal better described as inference, explanation, or prediction? Explain why.”</em></p> </blockquote> <hr/> <h2 id="step-2-select-candidate-models">Step 2: Select Candidate Models</h2> <p>Identify several possible models that could address the question, knowing you may discard some later.</p> <p><strong>Examples for electrical fire data:</strong></p> <ul> <li>Mean number of fires by building age group</li> <li>Line of best fit between wiring age and incident frequency</li> <li>Correlation between wire material and fire occurrence</li> <li>Simple regression predicting fire count from building age</li> </ul> <p><strong>Prompting technique:</strong> Few-shot prompting</p> <blockquote> <p><em>“Here is my data and my research questions</em><br/> <em>Based on those, suggest appropriate models for investigating electrical fire causes.”</em></p> </blockquote> <hr/> <h2 id="step-3-evaluate-model-fit-and-usefulness">Step 3: Evaluate Model Fit and Usefulness</h2> <p>Test each model by asking:</p> <ul> <li>Does this model help answer my question?</li> <li>Is the result interpretable in a real-world fire safety context?</li> <li>Does it respect the limits of the data?</li> </ul> <p>Models that do not add insight are set aside, not forced into the analysis.</p> <p><strong>Prompting technique:</strong> Chain-of-thought prompting</p> <blockquote> <p><em>“Step by step, evaluate whether this model provides meaningful evidence for the research question about electrical fire risk.”</em></p> </blockquote> <hr/> <h2 id="step-4-consider-variability-and-uncertainty">Step 4: Consider Variability and Uncertainty</h2> <p>No model perfectly represents reality. So, eexamine:</p> <ul> <li>Spread and variability in the data</li> <li>Sensitivity to outliers</li> <li>Whether small sample sizes limit conclusions</li> </ul> <p>This step helps avoid overconfident claims.</p> <p><strong>Prompting technique:</strong> Self-consistency</p> <blockquote> <p><em>“Analyze this model’s results twice [insert], focusing once on patterns and once on uncertainty.</em><br/> <em>Do both interpretations support the same conclusion?”</em></p> </blockquote> <hr/> <h2 id="step-5-balance-interpretability-and-performance">Step 5: Balance Interpretability and Performance</h2> <p>Decide how much interpretability matters for the investigation.</p> <p><strong>For electrical fire analysis:</strong></p> <p>Simple, interpretable models (e.g., averages, linear trends) often matter more than high-performance black-box models.</p> <p>However, more complex models can sometimes reveal upper bounds or hidden structure.</p> <p>Choose models that align with the purpose of the investigation, not just technical sophistication.</p> <p><strong>Prompting technique:</strong> Prompt chaining</p> <p><strong>Prompt 1:</strong></p> <blockquote> <p><em>“Explain this model in plain language for a community safety report.”</em></p> </blockquote> <p><strong>Prompt 2:</strong></p> <blockquote> <p><em>“Now explain what this model cannot tell us.”</em></p> </blockquote> <hr/> <h2 id="step-6-iterate-when-needed">Step 6: Iterate When Needed</h2> <p>If modeling reveals gaps or limitations, return to earlier phases:</p> <ul> <li>Collect or include additional variables (e.g., building use type)</li> <li>Transform variables (e.g., age categories)</li> <li>Revisit filtering or grouping decisions</li> </ul> <p>Modeling informs what to do next, not just what to conclude.</p> <hr/> <h2 id="key-takeaway">Key Takeaway</h2> <p>Models are <strong>evidence-building tools, not final answers</strong>. In electrical fire investigations, choosing the right model means balancing clarity, uncertainty, and real-world relevance.</p> <p><strong>LLMs can assist by:</strong></p> <ul> <li>Suggesting model options</li> <li>Explaining assumptions</li> <li>Helping critique interpretability</li> </ul> <p>But deciding which models matter and why is an essential part of data sensemaking, and remains the your responsibility.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="statistical models"/><category term="data modeling"/><category term="science education"/><category term="data literacy"/><category term="inference"/><summary type="html"><![CDATA[Learning to select, evaluate, and interpret models as evidence-building tools that balance clarity, uncertainty, and real-world relevance.]]></summary></entry><entry><title type="html">Data Exploration and Visualization</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/data-exploration/" rel="alternate" type="text/html" title="Data Exploration and Visualization"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/data-exploration</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/data-exploration/"><![CDATA[<h2 id="context-electrical-fire-incidents-and-contributing-factors">Context: Electrical Fire Incidents and Contributing Factors</h2> <p>Throughout this module, we continue working with our running example of investigating electrical fire incidents, focusing on how visualization helps reveal patterns and relationships that inform our understanding of contributing factors.</p> <hr/> <h2 id="why-this-phase-matters">Why This Phase Matters</h2> <p>Once data has been cleaned and structured, the next step is to explore it visually. Exploration is not about proving a conclusion, it is about <strong>discovering patterns, relationships, anomalies, and uncertainties</strong> that help refine understanding of a real-world problem.</p> <p>In investigations of electrical fires, visualization helps reveal:</p> <ul> <li><strong>Trends over time</strong> (e.g., changes in incidents by year)</li> <li><strong>Differences across categories</strong> (e.g., wire material, building age)</li> <li><strong>Relationships among variables</strong> (e.g., wiring age and fire frequency)</li> </ul> <p>This phase is closely connected to modeling and interpretation. What you see here may lead you to revise your research question, request additional data, or rethink earlier assumptions.</p> <hr/> <h2 id="a-critical-stance-toward-llms">A Critical Stance Toward LLMs</h2> <p>Throughout this phase, LLMs function as <strong>idea generators and explanation partners, not authorities</strong>.</p> <p>Any chart, interpretation, or narrative produced with LLM assistance must be:</p> <ul> <li>Verified against the dataset</li> <li>Checked for misleading design choices</li> <li>Documented and cited when appropriate</li> </ul> <hr/> <h2 id="exploration-and-visualization-workflow">Exploration and Visualization Workflow</h2> <p>The following seven-step workflow guides the process of exploring data visually and communicating findings responsibly.</p> <hr/> <h2 id="step-1-clarify-the-visualization-goal">Step 1: Clarify the Visualization Goal</h2> <p>Before creating any chart, you may restate my investigative question and decide what kind of insight you are looking for.</p> <p><strong>Electrical fire examples:</strong></p> <ul> <li>Are electrical fires more common in older buildings?</li> <li>Do certain wire materials appear more frequently in fire incidents?</li> <li>Has the number of electrical fires increased or decreased over time?</li> </ul> <p>Each question implies a different visualization goal:</p> <ul> <li><strong>Comparison</strong> (e.g., fires by wire material)</li> <li><strong>Trend</strong> (e.g., fires per year)</li> <li><strong>Distribution</strong> (e.g., building ages involved in fires)</li> <li><strong>Relationship</strong> (e.g., wiring age vs. fire occurrence)</li> </ul> <p><strong>Prompting technique:</strong> Zero-shot prompting</p> <blockquote> <p><em>“Given this research question about electrical fire causes, what type of insight should a visualization aim to show: trend, comparison, distribution, or relationship?”</em></p> </blockquote> <hr/> <h2 id="step-2-select-chart-type-and-map-variables">Step 2: Select Chart Type and Map Variables</h2> <p>Once the goal is clear, choose a chart that matches it and decide how variables map to:</p> <ul> <li>Axes (x, y)</li> <li>Color or grouping</li> <li>Size or facets</li> <li>Time windows or aggregation levels</li> </ul> <p><strong>Examples:</strong></p> <ul> <li><strong>Bar chart:</strong> number of fires by wire material</li> <li><strong>Line chart:</strong> yearly count of electrical fires</li> <li><strong>Scatterplot:</strong> building age vs. number of incidents</li> </ul> <p><strong>Prompting technique:</strong> Few-shot prompting</p> <blockquote> <p><em>“Here is my data on fire safety analysis.</em><br/> <em>Based on these, suggest an appropriate chart for comparing electrical fire incidents across wire materials.”</em></p> </blockquote> <p>I treat the output as a suggestion, not a final decision.</p> <hr/> <h2 id="step-3-build-a-first-draft-visualization">Step 3: Build a First-Draft Visualization</h2> <p>At this stage, the goal is <strong>exploration, not polish</strong>.</p> <p>Generate an initial chart and ask:</p> <ul> <li>What stands out?</li> <li>What surprises me?</li> <li>What looks unclear or suspicious?</li> </ul> <p><strong>Prompting technique:</strong> Prompt chaining</p> <p><strong>Prompt 1:</strong></p> <blockquote> <p><em>“Create a simple bar chart comparing electrical fire incidents by wire material.”</em></p> </blockquote> <p><strong>Prompt 2:</strong></p> <blockquote> <p><em>“Explain what patterns or differences are most noticeable in this chart.”</em></p> </blockquote> <hr/> <h2 id="step-4-interpret-what-the-data-shows">Step 4: Interpret What the Data Shows</h2> <p>Interpretation involves describing what the data actually displays, not explaining why it happens.</p> <p>Separate:</p> <ul> <li><strong>Observed patterns</strong> (what the data shows)</li> <li><strong>Speculation or hypotheses</strong> (possible explanations)</li> </ul> <p><strong>Example interpretation:</strong></p> <p><strong>Observed:</strong> “Buildings with wiring older than 30 years appear more frequently in the dataset.”</p> <p><strong>Speculation:</strong> “Older insulation materials may degrade over time, increasing fire risk.”</p> <p><strong>Prompting technique:</strong> Chain-of-thought prompting</p> <blockquote> <p><em>“Step by step, describe the main patterns in this electrical fire visualization.</em><br/> <em>Clearly separate observed data patterns from possible explanations. Use the attached data as context”</em></p> </blockquote> <hr/> <h2 id="step-5-critique-the-visualization">Step 5: Critique the Visualization</h2> <p>Before communicating results, check whether the visualization could mislead.</p> <p><strong>Key questions:</strong></p> <ul> <li>Are axes scaled appropriately?</li> <li>Is any data hidden by aggregation?</li> <li>Are categories uneven or misleading?</li> <li>Is important context missing?</li> </ul> <p><strong>Prompting technique:</strong> Self-consistency</p> <blockquote> <p><em>“Review this visualization for potential sources of misinterpretation.</em><br/> <em>Then review it again from the perspective of a skeptical reader.”</em></p> </blockquote> <p>This helps surface issues you might overlook.</p> <hr/> <h2 id="step-6-communicate-findings">Step 6: Communicate Findings</h2> <p>Once the visualization is accurate and interpretable, translate it into communication products:</p> <ul> <li>A short written results section</li> <li>A slide or poster figure</li> <li>A one-page infographic or handout</li> </ul> <p><strong>Each figure includes:</strong></p> <ul> <li>A clear title</li> <li>Labeled axes with units</li> <li>A concise caption explaining what the visualization shows</li> <li>Notes on data source and processing decisions</li> </ul> <hr/> <h2 id="step-7-review-revise-and-archive">Step 7: Review, Revise, and Archive</h2> <p>Data visualization is rarely final on the first attempt.</p> <p>You should:</p> <ul> <li>Share the visualization for peer feedback</li> <li>Revise both the chart and interpretation</li> <li>Archive the final visualization, dataset, and a brief README describing how it was created</li> </ul> <p>This ensures transparency and reproducibility.</p> <hr/> <h2 id="key-takeaway">Key Takeaway</h2> <p>Exploration and visualization are not just about presentation, they are <strong>thinking tools</strong>. In investigating electrical fire data, visualizations help surface relationships, challenge assumptions, and guide next steps in analysis.</p> <p>LLMs can accelerate this process by suggesting options and explanations, but the responsibility for accuracy, interpretation, and ethical use always remains with the investigator.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="data visualization"/><category term="data exploration"/><category term="science education"/><category term="data literacy"/><category term="charts"/><summary type="html"><![CDATA[Using visualization as a thinking tool to discover patterns, relationships, and uncertainties in science data investigations.]]></summary></entry><entry><title type="html">Data Wrangling (Processing &amp;amp; Transforming Data)</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/data-wrangling/" rel="alternate" type="text/html" title="Data Wrangling (Processing &amp;amp; Transforming Data)"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/data-wrangling</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/data-wrangling/"><![CDATA[<h2 id="why-data-wrangling-matters">Why Data Wrangling Matters</h2> <p>Real data is almost never ready for analysis. Measurements are missing, values are inconsistent, units don’t match, and datasets include information that doesn’t actually help answer your question.</p> <p><strong>Data wrangling</strong> is the process of cleaning, organizing, and transforming data so it becomes usable for sensemaking. This phase is where you decide what data matters, how it should be structured, and what transformations best support your investigation.</p> <p>In practice, this is also where most of the work happens.</p> <hr/> <h2 id="what-you-are-doing-in-this-phase">What You Are Doing in This Phase</h2> <p>Data wrangling is not a checklist; it is <strong>iterative decision-making</strong>. At each step, you:</p> <ul> <li>Inspect the data</li> <li>Decide what to keep, change, or remove</li> <li>Justify those decisions based on your investigation goals</li> </ul> <p>LLMs can help by explaining options, consequences, and tradeoffs, but you remain responsible for the final choices.</p> <p><strong>By the end of this phase, you should have:</strong></p> <ul> <li>A cleaned and well-structured dataset</li> <li>Transformed variables that support your research question</li> <li>A table of descriptive statistics</li> <li>A clear rationale for each wrangling decision</li> </ul> <hr/> <h2 id="core-data-wrangling-actions">Core Data Wrangling Actions</h2> <p>The following five actions represent the fundamental work of data wrangling. Each requires scientific judgment and intentional decision-making.</p> <hr/> <h2 id="1-filtering-focus-on-what-matters">1. Filtering: Focus on What Matters</h2> <p><strong>Filtering</strong> means selecting only the subset of data relevant to your question.</p> <p>This helps reduce noise and keeps the analysis aligned with the phenomenon you are studying.</p> <p><strong>Science examples:</strong></p> <ul> <li>Keeping only temperature readings from one location</li> <li>Selecting trials conducted under the same experimental conditions</li> </ul> <p><strong>LLM support (Zero-shot or Few-shot prompting):</strong></p> <p>Use the LLM model to suggest reasonable filtering criteria based on your research question, then decide whether they make scientific sense.</p> <hr/> <h2 id="2-grouping-create-meaningful-comparisons">2. Grouping: Create Meaningful Comparisons</h2> <p><strong>Grouping</strong> involves organizing data into categories or levels so comparisons are possible.</p> <p>This is essential for identifying patterns and differences.</p> <p><strong>Science examples:</strong></p> <ul> <li>Grouping electrical resistance data by wire material</li> <li>Grouping biology measurements by species or treatment group</li> <li>Grouping math test scores by problem type</li> </ul> <p><strong>LLM support (Few-shot prompting):</strong></p> <p>Provide examples of how grouping is used in similar investigations and ask the LLM model to suggest groupings, but validate them against your data context.</p> <hr/> <h2 id="3-data-cleaning-address-messiness-explicitly">3. Data Cleaning: Address Messiness Explicitly</h2> <p><strong>Cleaning</strong> means identifying and deciding how to handle:</p> <ul> <li>Missing values</li> <li>Outliers</li> <li>Duplicates</li> <li>Inconsistent units or labels</li> </ul> <p>There is rarely one “correct” cleaning choice—each decision affects interpretation.</p> <p><strong>Science examples:</strong></p> <ul> <li>Deciding whether to remove a temperature reading far outside the rest</li> <li>Choosing how to handle a missing measurement in a lab trial</li> <li>Converting all lengths to meters for consistency</li> </ul> <p><strong>LLM support (Chain-of-thought prompting):</strong></p> <p>Ask the LLM model to walk through possible cleaning options and their consequences. Use this to compare alternatives, not to outsource judgment.</p> <hr/> <h2 id="4-data-transformation-make-data-more-useful">4. Data Transformation: Make Data More Useful</h2> <p><strong>Transformation</strong> involves creating new variables or modifying existing ones to better support analysis.</p> <p>This step often connects raw measurements to scientific meaning.</p> <p><strong>Science examples:</strong></p> <ul> <li>Calculating speed from distance and time</li> <li>Creating ratios (e.g., mass per unit volume)</li> <li>Converting continuous values into categories (bins)</li> <li>Aggregating daily measurements into weekly averages</li> </ul> <p><strong>LLM support (Prompt chaining):</strong></p> <p><strong>First prompt:</strong> Identify useful transformations.</p> <p><strong>Second prompt:</strong> Explain how each transformation would support the investigation.</p> <hr/> <h2 id="5-summarizing-describe-the-data-before-explaining-it">5. Summarizing: Describe the Data Before Explaining It</h2> <p>Before interpreting patterns, you need a descriptive overview of the dataset.</p> <p><strong>Common summaries include:</strong></p> <ul> <li>Mean, median, mode</li> <li>Minimum and maximum</li> <li>Range and standard deviation</li> </ul> <p>These summaries help you understand variability and typical values.</p> <p><strong>Science examples:</strong></p> <ul> <li>Average reaction rate across trials</li> <li>Variation in plant height within a treatment group</li> <li>Spread of resistance values for different wire lengths</li> </ul> <p><strong>LLM support (Self-consistency prompting):</strong></p> <p>Ask the LLM model to generate summaries multiple times or in different formats to check for stability and consistency in interpretation.</p> <hr/> <h2 id="a-critical-reminder">A Critical Reminder</h2> <p>Data wrangling is not neutral. Every choice, what to remove, how to group, what to transform, shapes the story the data can tell.</p> <p><strong>LLMs are most useful here as thinking partners:</strong></p> <ul> <li>Explaining options</li> <li>Surfacing tradeoffs</li> <li>Checking reasoning</li> </ul> <p>They do not decide what is scientifically appropriate, <strong>you do</strong>.</p> <hr/> <p>In the next module, we move from preparing data to analyzing it, where patterns become evidence and claims require justification.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="data wrangling"/><category term="data cleaning"/><category term="data transformation"/><category term="science education"/><category term="data literacy"/><summary type="html"><![CDATA[Learning to clean, organize, and transform data through iterative decision-making to prepare it for meaningful scientific analysis.]]></summary></entry><entry><title type="html">Frame the Problem</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/framing-the-problem/" rel="alternate" type="text/html" title="Frame the Problem"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/framing-the-problem</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/framing-the-problem/"><![CDATA[<h2 id="why-framing-the-problem-comes-first">Why Framing the Problem Comes First</h2> <p>Before we graph data, calculate averages, or run statistical tests, we need to step back and ask:</p> <p><strong>What problem are we actually trying to understand?</strong></p> <p>Framing the problem anchors a data investigation in real-world phenomena. According to Lee et al. (2022), effective data investigations do not begin with tools or techniques; they begin with context, purpose, and questions worth answering. This framing phase shapes every decision that follows, from the research question we pose to the data we choose to analyze.</p> <p>Importantly, this is not a one-time step. Throughout a data investigation, we return to the framing phase to ensure our analysis still aligns with the real-world problem we care about.</p> <hr/> <h2 id="learning-goals-for-this-module">Learning Goals for This Module</h2> <p>By the end of this module, teachers and students will be able to:</p> <ul> <li>Situate a data investigation in a real-world scientific context</li> <li>Move from broad issues → focused research questions</li> <li>Select or evaluate datasets that align with those questions</li> <li>Use LLMs productively to support (not replace) human sensemaking during framing</li> </ul> <hr/> <h2 id="step-1-identify-the-broader-issue">Step 1: Identify the Broader Issue</h2> <p>Data investigations begin with real-world phenomena, not datasets.</p> <p>At this stage, I will ask you to zoom out and identify:</p> <ul> <li>What is happening in the world?</li> <li>Why does it matter?</li> <li>Who is affected?</li> </ul> <p>This is where background information comes in. We are not analyzing data yet; we are building contextual understanding.</p> <h3 id="example-electrical-fires-physical-science--engineering-context">Example: Electrical Fires (Physical Science / Engineering Context)</h3> <p>Suppose we want to investigate electrical fires.</p> <p>The broader issue might include:</p> <ul> <li>Repeated electrical fires in a community</li> <li>Power outages affecting homes and businesses</li> <li>Safety risks and economic consequences</li> </ul> <p>Here, the investigation is grounded in a real system that could be improved, which emphasizes problem-solving in real-world context.</p> <h3 id="how-llms-may-support-this-step">How LLMs May Support This Step</h3> <p>LLMs are especially useful here for brainstorming and contextual exploration.</p> <p><strong>Example prompt (Zero-shot prompt):</strong></p> <blockquote> <p><em>“Act as a science teacher helping students understand a real-world problem.</em><br/> <em>What are some typical broader issues related to electrical fires in residential neighborhoods?”</em></p> </blockquote> <p>At this stage, the LLM helps surface possibilities, not answers. Teachers and students still decide which issues are relevant and meaningful in their context.</p> <hr/> <h2 id="step-2-clarify-why-the-problem-matters">Step 2: Clarify Why the Problem Matters</h2> <p>Once the broader issue is identified, the next step is to articulate why the investigation is important.</p> <p>This helps you move beyond “we have data” to “this data helps us understand something meaningful.”</p> <p>Questions that may be asked here include:</p> <ul> <li>What could change if we understood this problem better?</li> <li>What decisions might this data inform?</li> <li>How does this connect to science learning goals?</li> </ul> <h3 id="classroom-example">Classroom Example</h3> <p>In the electrical fire case:</p> <ul> <li>Understanding causes could inform safer building practices</li> <li>Data could help prioritize inspections or upgrades</li> <li>The investigation connects to concepts like conductivity, resistance, and materials science</li> </ul> <h3 id="using-llms-thoughtfully">Using LLMs Thoughtfully</h3> <p><strong>Example prompt (Zero-shot prompt):</strong></p> <blockquote> <p><em>“Why might understanding the causes of electrical fires be important for community safety and engineering decisions?”</em></p> </blockquote> <p>This supports sensemaking, but students should still discuss and refine the importance in their own words.</p> <hr/> <h2 id="step-3-develop-investigative-research-questions">Step 3: Develop Investigative (Research) Questions</h2> <p>Only after understanding the broader issue do we narrow our focus.</p> <p>Framing the problem includes determining what focus is most productive. Sometimes, the initial problem needs to be reframed into a question that is more answerable with available data.</p> <h3 id="from-broad-issue-to-research-question">From Broad Issue to Research Question</h3> <p><strong>Broad issue:</strong></p> <blockquote> <p><em>Electrical fires are increasing in residential areas.</em></p> </blockquote> <p><strong>Focused investigative question:</strong></p> <blockquote> <p><em>“What types of electrical wire materials are most commonly associated with electrical fire incidents?”</em></p> </blockquote> <p>This question:</p> <ul> <li>Is specific</li> <li>Can be answered using data</li> <li>Connects directly to scientific concepts (materials, conductivity, heat)</li> </ul> <h3 id="using-llms-to-refine-questions">Using LLMs to Refine Questions</h3> <p>LLMs are particularly useful for question refinement, not question generation alone.</p> <p><strong>Example prompt (Zero-shot prompt):</strong></p> <blockquote> <p><em>“Here is a broad issue: electrical fires in residential buildings.</em><br/> <em>Suggest three possible data-driven research questions that could be investigated using existing datasets.”</em></p> </blockquote> <p>Teachers and students then evaluate:</p> <ul> <li>Which question is measurable?</li> <li>Which aligns with the data we can access?</li> <li>Which best supports our learning goals?</li> </ul> <hr/> <h2 id="step-4-identify-and-evaluate-available-data">Step 4: Identify and Evaluate Available Data</h2> <p>Once a research question is defined, we ask:</p> <p><strong>What data would help us answer this question?</strong></p> <p>This includes:</p> <ul> <li>What variables are needed?</li> <li>What measurements matter?</li> <li>What limitations exist in the data?</li> </ul> <h3 id="example-continued">Example Continued</h3> <p><strong>Research question:</strong></p> <blockquote> <p><em>“What electrical wire materials are most commonly used in electrical fire incidents?”</em></p> </blockquote> <p>Possible datasets might include:</p> <ul> <li>Fire incident reports</li> <li>Building inspection records</li> <li>Material type and installation year</li> </ul> <p>Here, alignment matters. A dataset that lacks wire material information, even if large, is not useful for this question.</p> <h3 id="llms-as-data-selection-aids">LLMs as Data-Selection Aids</h3> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Given this research question [paste research question here], what types of data variables would be necessary to investigate it?”</em></p> </blockquote> <p>The LLM helps identify what to look for, but teachers and students still evaluate data quality and relevance.</p> <hr/> <h2 id="step-5-revisiting-the-frame-iterative-sensemaking">Step 5: Revisiting the Frame (Iterative Sensemaking)</h2> <p>A key point from Lee et al. (2022) is that framing the problem is <strong>iterative</strong>.</p> <p>As data is explored, we may realize:</p> <ul> <li>The question is too broad</li> <li>The data is incomplete</li> <li>A different question is more productive</li> </ul> <p>At this point, we return to framing, not as failure, but as scientific practice.</p> <p><strong>Example Reflection Prompt (Zero-shot prompting):</strong></p> <blockquote> <p><em>“Based on the available data, does this research question need to be refined? If so, suggest how.”</em></p> </blockquote> <p>This models authentic data science and scientific inquiry.</p> <hr/> <h2 id="closing-the-module">Closing the Module</h2> <p>In this module, I want teachers and students to see that data investigation begins long before analysis. Framing the problem shapes the questions we ask, the data we select, and the conclusions we draw.</p> <p>LLMs are powerful partners in this phase, but only when used to expand thinking, not replace it.</p> <p>In the next module, we move from framing to exploring and preparing data, where numbers, variability, and patterns take center stage.</p> <hr/> <p><strong>Reference:</strong></p> <p>Lee, V. R., Wilkerson, M. H., &amp; Lanouette, K. (2022). A call for a reimagined science education data literacy. <em>Journal of Research in Science Teaching</em>, 59(6), 1019-1049.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="data investigation"/><category term="problem framing"/><category term="science education"/><category term="data sensemaking"/><summary type="html"><![CDATA[Understanding how to anchor data investigations in real-world phenomena by framing problems before analysis begins.]]></summary></entry><entry><title type="html">Intro to Large Language Models</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/intro-to-llm/" rel="alternate" type="text/html" title="Intro to Large Language Models"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/intro-to-llm</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/intro-to-llm/"><![CDATA[<h2 id="why-this-module-matters">Why This Module Matters</h2> <p>Before asking teachers to use AI for data analysis and sensemaking, it is important to slow down and build a <strong>shared mental model</strong> of what Large Language Models (LLMs) actually are - and what they are not.</p> <p>This matters because how we understand these tools directly shapes:</p> <ul> <li>how we use them in instruction,</li> <li>how much we trust them, and</li> <li>how we design prompts for them.</li> </ul> <p>Particulary since the use case in this course is to make sense of science data, we need to develop some understanding, to some extent, of how they produce their responses.</p> <p>Without a clear mental model, it is easy to overestimate what LLMs can do or to misuse them in ways that undermine sensemaking rather than support it. Specifically, whatever the LLMs generate, are they grounded in the data we provide? Does the output help us better develop understanding of the phenomenon of interest?</p> <p><strong>Finally, and this is core to data sensemking, does the LLM help us create reliable ideas of reality from the observed data?</strong></p> <hr/> <h2 id="what-is-a-large-language-model-llm">What Is a Large Language Model (LLM)?</h2> <p>Large Language Models (LLMs) are a type of Artificial Intelligence (AI) designed to work with <strong>human language</strong>. They are trained on massive collections of text and learn <strong>statistical patterns in language</strong>, enabling them to generate new text that sounds coherent and contextually appropriate.</p> <p>In practical terms, LLMs can:</p> <ul> <li>numerical data</li> <li>respond to questions,</li> <li>summarize text,</li> <li>generate explanations,</li> <li>translate languages,</li> <li>write code, and</li> </ul> <p>Common examples of LLMs include:</p> <ul> <li><strong>ChatGPT</strong> (OpenAI)</li> <li><strong>Claude</strong> (Anthropic)</li> <li><strong>Gemini</strong> (Google)</li> <li><strong>LLaMA</strong> (Meta)</li> </ul> <p>Most LLM models teachers encounter are <strong>general-purpose models</strong>. This means they are designed to perform reasonably well across many tasks, rather than being experts in a single discipline such as statistics or data science.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/EmmanuelAdeloju/assets/img/prompt-480.webp 480w,/EmmanuelAdeloju/assets/img/prompt-800.webp 800w,/EmmanuelAdeloju/assets/img/prompt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/EmmanuelAdeloju/assets/img/prompt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Fig 1. A simplified view of how a prompt guides a Large Language Model to generate a response by predicting likely next words rather than reasoning from data.</figcaption> </figure> <hr/> <h2 id="llms-are-pattern-based">LLMs Are Pattern-Based</h2> <p>This is one of the most important ideas in this module.</p> <p>LLMs do not <em>understand</em> data, concepts, or scientific phenomena in the way humans do. Instead, they generate responses by predicting what words are most likely to come next based on patterns learned during training.</p> <p>When an LLM provides a convincing explanation of a graph or trend, it is <strong>not reasoning from first principles</strong>. It is drawing on patterns it has encountered in similar explanations across its training data.</p> <p>This is why LLM outputs can:</p> <ul> <li>sound confident even when incorrect,</li> <li>use appropriate scientific language without grounding claims in evidence, and</li> <li>miss contextual details that matter in classrooms.</li> </ul> <p>Understanding this limitation is essential for using LLMs productively in science education.</p> <p>Below is a visualization of how an LLM is checking through all possible words that should follow “work”. The word with similar/close “numeric value” will be assigned as the next word.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/EmmanuelAdeloju/assets/img/pattern-480.webp 480w,/EmmanuelAdeloju/assets/img/pattern-800.webp 800w,/EmmanuelAdeloju/assets/img/pattern-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/EmmanuelAdeloju/assets/img/pattern.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><span style="color: #f0f0f0;">Fig 2. LLMs seeking patterns to match work with the next possible word <d-cite key="ft-generative-ai">2</d-cite>.</span></figcaption> </figure> <hr/> <h2 id="how-do-llms-learn">How Do LLMs Learn?</h2> <p>Teachers do not need all the technical details—but they <em>do</em> need intuition. Understanding how these models are trained helps us interact with them more critically and productively.</p> <h3 id="training-at-a-high-level">Training at a High Level</h3> <p>LLMs are trained in three main stages.</p> <h3 id="stage-1-data-collection-and-preprocessing">Stage 1: Data Collection and Preprocessing</h3> <p>The model is first exposed to enormous amounts of text from sources such as:</p> <ul> <li>websites,</li> <li>Wikipedia,</li> <li>books,</li> <li>articles, and</li> <li>other publicly available text.</li> </ul> <p>This data is cleaned and filtered to remove low-quality or duplicate content. While this process is extensive, it is imperfect—meaning biases and gaps can remain.</p> <p><strong>Educational implication:</strong><br/> What the model “knows” depends entirely on what it has encountered in text form.</p> <h3 id="stage-2-pretraining-learning-language-patterns">Stage 2: Pretraining (Learning Language Patterns)</h3> <p>During pretraining, the model learns general patterns in language by predicting missing or next words across <strong>billions (or trillions) of words</strong>.</p> <p>For example, an earlier version of ChatGPT (GPT-3.5) was trained on approximately <strong>45 TB of compressed text before filtering</strong>, and about <strong>570 GB after filtering</strong> (Brown et al., 2020), representing hundreds of billions to trillions of words. Training on such broad and diverse text sources results in <strong>general-purpose models</strong> rather than deep expertise in any single discipline.</p> <p>For instance, if given the prompt:</p> <blockquote> <p><em>“When the temperature increases, the solubility of most solids in water…”</em></p> </blockquote> <p>The model learns that phrases like <em>“also increases”</em> are statistically likely to follow.</p> <p>This stage creates a model that can respond across many topics, but without disciplinary understanding.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/EmmanuelAdeloju/assets/img/llmprediction-480.webp 480w,/EmmanuelAdeloju/assets/img/llmprediction-800.webp 800w,/EmmanuelAdeloju/assets/img/llmprediction-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/EmmanuelAdeloju/assets/img/llmprediction.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Large Language Models generate responses by predicting the most probable next tokens based on patterns learned during training, not by reasoning from first principles.</figcaption> </figure> <h3 id="stage-3-fine-tuning-specialization">Stage 3: Fine-Tuning (Specialization)</h3> <p>After pretraining, models can be fine-tuned using smaller, more focused datasets or human feedback.</p> <p>Fine-tuning means adjusting a pretrained model so it performs better on specific types of tasks (e.g., education, coding, or scientific explanation). This process often relies on <strong>transfer learning</strong>—using general language knowledge as a foundation rather than learning from scratch.</p> <p><strong>Plain-language definition:</strong><br/> <em>Transfer learning</em> means starting with a broadly trained model and refining it for a specific purpose.</p> <p><strong>Educational implication:</strong><br/> Even “education-focused” AI tools still rely on general language patterns, not deep disciplinary understanding.</p> <hr/> <h2 id="prompting-and-prompt-engineering">Prompting and Prompt Engineering</h2> <h3 id="what-is-a-prompt">What Is a Prompt?</h3> <p>A prompt is the input we give an LLM—usually text—that describes:</p> <ul> <li>the task,</li> <li>the context, and</li> <li>the expected type of response.</li> </ul> <p><strong>Example prompt:</strong></p> <blockquote> <p><em>“Explain the trend shown in this graph to a 7th-grade student using evidence from the data.”</em></p> </blockquote> <h3 id="what-is-prompt-engineering">What Is Prompt Engineering?</h3> <p>Prompt engineering is the intentional design and refinement of prompts to:</p> <ul> <li>clarify the task,</li> <li>constrain the model’s response, and</li> <li>align outputs with instructional goals.</li> </ul> <p>In education, prompt engineering is less about “hacking” the model and more about <strong>pedagogical precision</strong>.</p> <p>I think of prompt engineering as <strong>lesson design for an AI collaborator</strong>.</p> <p>We will discuss Prompt Engineering more in Module 2.</p> <hr/> <h2 id="why-llms-can-be-helpful-for-science-data-sensemaking">Why LLMs Can Be Helpful for Science Data Sensemaking</h2> <p>Despite their limitations, LLMs can be powerful sensemaking supports when used carefully.</p> <p>They are especially useful for:</p> <ul> <li>translating data patterns into explanatory language,</li> <li>generating multiple interpretations of the same dataset,</li> <li>supporting claim–evidence–reasoning (CER),</li> <li>helping teachers anticipate student misconceptions.</li> </ul> <p>What they cannot do reliably is:</p> <ul> <li>validate conclusions,</li> <li>detect subtle data errors, or</li> <li>replace student reasoning or disciplinary judgment.</li> </ul> <hr/> <h2 id="major-risks-teachers-need-to-be-aware-of">Major Risks Teachers Need to Be Aware Of</h2> <p>To use LLMs responsibly, we need to be explicit about the risks.</p> <h3 id="attributing-human-like-reasoning">Attributing Human-Like Reasoning</h3> <p>Fluent language can easily be mistaken for genuine understanding, hence attributing human-level reasoning to it. This may lead to overconfidence in explanations that are not grounded in the data.</p> <h3 id="uncritical-trust-in-outputs">Uncritical Trust in Outputs</h3> <p>Repeated “good enough” responses can create a false sense of reliability. Without verification, incorrect interpretations may go unnoticed.</p> <h3 id="invisible-influence-on-thinking">Invisible Influence on Thinking</h3> <p>LLMs do not just answer questions—they <strong>frame problems</strong>. Their suggestions can subtly shape how data is interpreted or narrow the range of explanations considered.</p> <h3 id="displacement-of-collaborative-sensemaking">Displacement of Collaborative Sensemaking</h3> <p>Over-reliance on AI can reduce opportunities for productive struggle, discussion, and collaborative reasoning, all of which are essential in science learning.</p> <h3 id="erosion-of-analytical-judgment">Erosion of Analytical Judgment</h3> <p>When teachers or students defer too often to AI-generated interpretations, their ability to independently evaluate evidence and detect errors may weaken over time.</p> <hr/> <h2 id="closing-reflection">Closing Reflection</h2> <p>The goal of this module is not to convince you that LLMs are good or bad. Rather, it is to help you see these tools clearly—as powerful language-based systems that can support science data sensemaking <strong>only when humans remain firmly in the loop</strong>.</p> <p>This understanding sets the foundation for the next module, where we move from <em>what LLMs are</em> to <em>how we design prompts</em> that support meaningful data analysis in classrooms.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="LLMs"/><category term="AI literacy"/><category term="education"/><category term="data science"/><summary type="html"><![CDATA[Building a shared mental model of what Large Language Models actually are, and what they are not.]]></summary></entry><entry><title type="html">Prompt Engineering Framework for Data Sensemaking</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/prompt-framework/" rel="alternate" type="text/html" title="Prompt Engineering Framework for Data Sensemaking"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/prompt-framework</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/prompt-framework/"><![CDATA[<h2 id="what-is-data-sensemaking">What is Data Sensemaking?</h2> <p><strong>Data sensemaking</strong> is the process of engaging with data to explain, interpret, and understand a phenomenon, not just compute results.</p> <p>In the context of science education, data sensemaking involves understanding a phenomenon of interest and seeking certain outcomes from a given data. It moves beyond simple calculation to ask:</p> <ul> <li>What does this pattern mean?</li> <li>Why might it occur?</li> <li>What can we learn from this?</li> <li>What can we do with what we have learned?</li> </ul> <p>Central to using LLMs for data sensemaking is engaging in a cycle of questioning at every instance.</p> <p>Hence, when working with LLMs, prompt engineering becomes the method by which we structure our scientific inquiry necessary for data sensemaking, guiding the LLM model to found its responses in the data provided or stipulated instructions.</p> <hr/> <h2 id="the-sense-framework-for-data-sensemaking">The SENSE Framework for Data Sensemaking</h2> <p>I introduce <strong>SENSE</strong>, a prompt engineering framework grounded in core practices of scientific data sensemaking:</p> <p><strong>S</strong> — Specify the phenomenon<br/> <strong>E</strong> — Establish the data and variables<br/> <strong>N</strong> — Name the analytical task<br/> <strong>S</strong> — Support claims with evidence<br/> <strong>E</strong> — Examine limitations and uncertainty</p> <p>This framework helps teachers and students move from raw data to meaningful explanations that support understanding of phenomena of interest.</p> <hr/> <h2 id="1-specify-the-phenomenon">1. Specify the Phenomenon</h2> <p>Clearly state what real-world or scientific phenomenon the data represents.</p> <p>This anchors the model in <strong>meaning, not just numbers</strong>.</p> <p><strong>Example (Physics):</strong></p> <blockquote> <p><em>“This task is about how the period of a pendulum changes as string length increases.”</em></p> </blockquote> <p>By naming the phenomenon explicitly, you help the model connect calculations to real-world scientific concepts rather than treating the data as abstract numbers.</p> <hr/> <h2 id="2-establish-the-data-and-variables">2. Establish the Data and Variables</h2> <p>This involves giving as much information as you can about the data. From data provenance to the data types to the descriptions of the variables.</p> <p>Explicitly describe:</p> <ul> <li>What the data is about</li> <li>What type of data (ex. numerical or categorical?)</li> <li>What each variable represents</li> <li>Units of measurement</li> <li>Sample size or conditions (if known)</li> </ul> <p>This prevents the model from inventing assumptions. Most of the above information usually accompany secondary data you collect. In the case where you are working with a primary data (you or your student collected from experiment), then you will have to define these data descriptions. These are not by-tasks! They are core to getting the best sensemaking experience with LLMs.</p> <p><strong>Example (Chemistry):</strong></p> <blockquote> <p><em>“The dataset includes two columns which are temperature (°C) and reaction rate (mol/s) measured across five trials. The data contains information of an experiment carried out in a grade 10 chemistry class focused on how temperature affect the rate of decomposition of sodium carbonate on reaction with hydrochloric acid. “</em></p> </blockquote> <p>It is also important to have inspected the data to observe any anomaly or insight that may help the LLM have better understand the data. For example, you may notice that some of the temperature values are categorical (for example, instead of “2”, you see “two”). This is an imputation error you want to fix or ask the LLM to fix when the time comes.</p> <p>Essentially, providing context ensures the LLM model grounds its reasoning in the actual structure and constraints of your data.</p> <hr/> <h2 id="3-name-the-analytical-task">3. Name the Analytical Task</h2> <p>State exactly what kind of sensemaking is required, not just “analyze.”</p> <p><strong>Examples of tasks:</strong></p> <ul> <li>Identify patterns or trends</li> <li>Compare groups</li> <li>Quantify relationships</li> <li>Interpret variability</li> </ul> <p>This involves any statistical tests (t-test, ANOVA, Correlation test) you want to run or models (linear regression, logistic regression, time series) you want to build.</p> <p><strong>Example (Biology):</strong></p> <blockquote> <p><em>“Identify the correlation between light intensity and plant growth rate.”</em></p> </blockquote> <p>Being specific about the analytical task helps the model focus its execution on the appropriate type of pattern recognition or comparison to make.</p> <hr/> <h2 id="4-support-claims-with-evidence">4. Support Claims with Evidence</h2> <p>Require the model to tie every claim to the data, using numbers or comparisons. You want to make sure every output from the LLM model is rooted in the data. Remember LLMs hallucinate! If you want to make sense of data, you want every computation and insight to be true to the data and lead to genuine scientific understanding. You must be able to trace insight to the data.</p> <p>This mirrors <strong>scientific argumentation</strong>.</p> <p><strong>Example (Mathematics / Statistics):</strong></p> <blockquote> <p><em>“Citing specific values or variables from the dataset to support each claim you make, do this …]”</em></p> </blockquote> <p>This step ensures that explanations are grounded in data/evidence rather than general statements that could apply to any dataset or cannot be traced.</p> <hr/> <h2 id="5-examine-limitations-and-uncertainty">5. Examine Limitations and Uncertainty</h2> <p>Ensuring you audit the outputs of the LLM is key. Indeed, latest LLM models can self-audit and tell you where they might have made some errors. Also, you have the duty as someone with subject matter expertise to audit the responses and respond accordingly.</p> <p>Prompt reflection on:</p> <ul> <li>Measurement error</li> <li>Sample size</li> <li>Missing variables</li> <li>Alternative explanations</li> </ul> <p>This step helps you engage the LLM on verifying the outputs and fixing any possible error. Remember again that LLMs hallucinate, so you want to make sure this step is done.</p> <p><strong>Example:</strong></p> <blockquote> <p><em>“Identify at least one limitation of the dataset that affects how confidently we can interpret the results.”</em></p> </blockquote> <p>By explicitly asking for limitations, you encourage critical thinking about the strength of conclusions and the boundaries of what the data can support.</p> <hr/> <h2 id="full-sense-prompt-example">Full SENSE Prompt Example</h2> <p><strong>Context:</strong> Middle school science — electrical resistance</p> <blockquote> <p><em>“The data represents measurements of electrical resistance (ohms) for wires of different lengths (meters). The dataset includes wire length and resistance values from one classroom experiment. Analyze the relationship between wire length and resistance. Use specific data values to support your explanation. Finally, identify one limitation of this dataset that affects the strength of your conclusion.”</em></p> </blockquote> <p><strong>NOTE: The above prompt is encompassing and may not be always suitable to give a one-shot prompt. So you may breakdown the prompt as a form of “Prompt Chaining”</strong> <strong>Breaking down the SENSE components:</strong></p> <p><strong>S — Specify the phenomenon:</strong><br/> “measurements of electrical resistance (ohms) for wires of different lengths (meters)”</p> <p><strong>E — Establish the data and variables:</strong><br/> “wire length and resistance values from one classroom experiment”</p> <p><strong>N — Name the analytical task:</strong><br/> “Analyze the relationship between wire length and resistance”</p> <p><strong>S — Support claims with evidence:</strong><br/> “Use specific data values to support your explanation”</p> <p><strong>E — Examine limitations and uncertainty:</strong><br/> “identify one limitation of this dataset that affects the strength of your conclusion”</p> <p>This integrated prompt guides the LLM through a complete cycle of scientific reasoning, from phenomenon to evidence to critical reflection.</p> <hr/> <h2 id="moving-forward">Moving Forward</h2> <p>The SENSE framework provides a structured approach to prompt engineering that supports data sensemaking. By systematically incorporating these five elements, teachers can design prompts that support genuine data sensemaking rather than superficial pattern description.</p> <p>In practice, not every prompt needs all five elements, but being intentional about which elements to include helps ensure that LLM outputs support rather than replace scientific reasoning.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="LLMs"/><category term="prompt engineering"/><category term="data sensemaking"/><category term="science education"/><category term="SENSE framework"/><summary type="html"><![CDATA[Introducing SENSE, a structured framework for designing prompts that support scientific data sensemaking with Large Language Models.]]></summary></entry><entry><title type="html">Prompting Techniques for Science Data Analysis and Sensemaking</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/prompt-techniques/" rel="alternate" type="text/html" title="Prompting Techniques for Science Data Analysis and Sensemaking"/><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2026/prompt-techniques</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2026/prompt-techniques/"><![CDATA[<h2 id="understanding-prompt-engineering">Understanding Prompt Engineering</h2> <p>Before we dive into specific techniques, let me briefly remind you of what prompt engineering is, from a slightly different angle that may deepen your understanding. Prompt engineering is the design of tasks in natural language to LLMs to produce desired outputs.</p> <p>An important note: these prompt engineering guidelines work across different LLM models and tools. Whether you’re using ChatGPT, Claude, Gemini, or another model, these principles remain consistent.</p> <h2 id="components-of-a-prompt">Components of a Prompt</h2> <p>Before writing effective prompts, we need to understand their building blocks. Every well-designed prompt contains some combination of these components:</p> <p><strong>Instruction:</strong> An instruction or task you want the model to do</p> <p><strong>Context:</strong> Additional information you add to your instruction to make the model give better output</p> <p><strong>Input data:</strong> The question or task you are asking the model</p> <p><strong>Output indicator:</strong> How you want the output to be</p> <p>Understanding these components allows you to intentionally craft prompts that guide the model toward the kind of response you need.</p> <hr/> <h2 id="the-role-of-context-in-prompting">The Role of Context in Prompting</h2> <p>In prompt writing, <strong>context</strong> is the background information and framing you give the model so it knows how to approach a task, not just what calculation to perform.</p> <p>When you ask an LLM to calculate the mean of 5 numbers, the bare question might be:</p> <blockquote> <p><em>“What is the mean of these 5 numbers: 4, 7, 9, 10, 15?”</em></p> </blockquote> <p>Context is everything extra you add around that, such as:</p> <ul> <li>who the explanation is for,</li> <li>how detailed it should be, and</li> <li>what format to use.</li> </ul> <h3 id="example-without-much-context">Example Without Much Context</h3> <blockquote> <p><em>“Find the mean of these 5 numbers: 4, 7, 9, 10, 15.”</em></p> </blockquote> <p>The model knows it should compute the mean, but it doesn’t know:</p> <ul> <li>if it should show steps,</li> <li>explain the concept, or</li> <li>just output the final number.</li> </ul> <p>And even: mean of what numbers? Temperature or number of eggs?</p> <h3 id="example-with-richer-context">Example With Richer Context</h3> <blockquote> <p><em>“You are a friendly math tutor helping a 6th-grade student.</em> <em>Calculate the mean of the temperatures measured in five days: 4, 7, 9, 10, 15.</em> <em>First, show how to add the numbers, then divide by how many numbers there are, and finally give the answer in a full sentence the student can understand.”</em></p> </blockquote> <p>Here, the context includes:</p> <ul> <li><strong>Role:</strong> “friendly math tutor”</li> <li><strong>Audience:</strong> “6th-grade student”</li> <li><strong>Process:</strong> “show how to add,” then “divide by how many numbers,” then “give the answer in a full sentence”</li> </ul> <p>All of that shapes how the model performs the same core task: calculating the mean of 5 numbers.</p> <hr/> <h2 id="prompting-techniques-mapped-to-blooms-taxonomy">Prompting Techniques Mapped to Bloom’s Taxonomy</h2> <p>At this point, I assume we can agree that <strong>how we ask an LLM to work with data matters</strong>.</p> <p>So, let me introduce several prompting techniques and, more importantly, help you decide which technique fits which kind of data task.</p> <blockquote> <p><strong>Important note:</strong><br/> I do not recommend using all of these techniques all the time. Each technique is best suited for specific levels of task complexity, which I map explicitly to <strong>Bloom’s Taxonomy</strong> so teachers can make intentional choices.</p> </blockquote> <hr/> <h2 id="1-zero-shot-prompting">1. Zero-Shot Prompting</h2> <p><strong>Zero-shot prompting</strong> means asking the model to complete a task without providing any examples of how to do it.</p> <p>You give:</p> <ul> <li>The task</li> <li>The data</li> <li>Minimal framing</li> </ul> <h3 id="best-use-blooms-taxonomy">Best Use (Bloom’s Taxonomy)</h3> <ul> <li><strong>Remember</strong></li> <li><strong>Understand</strong></li> </ul> <p>This technique works best for simple, well-defined tasks where the goal is recall, identification, or basic interpretation.</p> <h3 id="physics-task-example">Physics Task Example</h3> <blockquote> <p><em>“What is a projectile?”</em></p> </blockquote> <p>This task is entirely about remembering and recalling. No need for any reasoning or evaluation.</p> <hr/> <h2 id="2-few-shot-prompting">2. Few-Shot Prompting</h2> <p><strong>Few-shot prompting</strong> provides one or more examples before asking the model to perform a new but similar task (Brown et al., 2020).</p> <p>This helps the model infer:</p> <ul> <li>The type of reasoning expected</li> <li>The structure of the response</li> </ul> <h3 id="best-use-blooms-taxonomy-1">Best Use (Bloom’s Taxonomy)</h3> <ul> <li><strong>Understand</strong></li> <li><strong>Apply</strong></li> </ul> <p>This technique is ideal for patterned data tasks or where students have to apply what they have learned to new forms of the same problem space.</p> <h3 id="chemistry-task-example">Chemistry Task Example</h3> <blockquote> <p><em>“Example:</em><br/> <em>Data: Temperature (°C): 10, 20, 30</em><br/> <em>Solubility (g): 15, 25, 35</em><br/> <em>Interpretation: As temperature increases, solubility increases.</em></p> <p><em>Now analyze this data:</em><br/> <em>Temperature (°C): 15, 12, 10</em><br/> <em>Solubility (g): 18, 12, 8</em><br/> <em>Describe the pattern.”</em></p> </blockquote> <p>The model sees the example and learns from that. Then mirrors the pattern to answer the new task.</p> <hr/> <h2 id="3-chain-of-thought-prompting">3. Chain-of-Thought Prompting</h2> <p><strong>Chain-of-thought prompting</strong> explicitly asks the model to show its (intermediate) reasoning steps (Wei et al., 2022).</p> <p>Use this sparingly and intentionally, especially when modeling reasoning.</p> <h3 id="best-use-blooms-taxonomy-2">Best Use (Bloom’s Taxonomy)</h3> <ul> <li><strong>Apply</strong></li> <li><strong>Analyze</strong></li> </ul> <p>This technique is best for multi-step data analysis, where the reasoning process matters as much as the answer.</p> <h3 id="biology-task-example">Biology Task Example</h3> <blockquote> <p><em>“A student measured heart rate (beats/min) before and after exercise:</em><br/> <em>Before: 72, 75, 73</em><br/> <em>After: 110, 115, 112</em><br/> <em>Calculate the mean for each condition and explain, step by step, what the data suggests about the effect of exercise.”</em></p> </blockquote> <p>In fact, there are two variations of chain-of-thought prompting:</p> <ul> <li><strong>Zero-shot CoT:</strong> Ask the model to reason step by step without providing examples.</li> <li><strong>Few-shot CoT:</strong> Provide a few step-by-step examples in the prompt to guide the model’s reasoning.</li> </ul> <hr/> <h2 id="4-self-consistency-prompting">4. Self-Consistency Prompting</h2> <p><strong>Self-consistency</strong> involves asking the model to generate multiple independent interpretations and then compare them (Wang et al., 2022).</p> <p>The goal is not agreement, but <strong>range and uncertainty</strong>.</p> <h3 id="best-use-blooms-taxonomy-3">Best Use (Bloom’s Taxonomy)</h3> <ul> <li><strong>Analyze</strong></li> <li><strong>Evaluate</strong></li> </ul> <p>This technique is especially useful for ambiguous or noisy data.</p> <h3 id="environmental-science-task-example">Environmental Science Task Example</h3> <blockquote> <p><em>“Based on this dataset showing CO₂ levels and temperature over time, generate three possible interpretations.</em><br/> <em>Then identify which interpretation is best supported by the data and explain why.”</em></p> </blockquote> <p>You may also paste the exact prompt into your LLM multiple times and compare the outputs and pick the majority answer. Your discretion is important here too—sometimes, the majority response will also be wrong.</p> <hr/> <h2 id="5-prompt-chaining">5. Prompt Chaining</h2> <p><strong>Prompt chaining</strong> breaks a complex task into a sequence of smaller prompts, where each output feeds into the next.</p> <p>This mirrors how we scaffold student thinking.</p> <h3 id="best-use-blooms-taxonomy-4">Best Use (Bloom’s Taxonomy)</h3> <ul> <li><strong>Apply</strong></li> <li><strong>Analyze</strong></li> <li><strong>Evaluate</strong></li> </ul> <p>This is ideal for extended data investigations.</p> <h3 id="science-data-example-mathematics--physics">Science Data Example (Mathematics / Physics)</h3> <p>In different prompts, do these:</p> <p><strong>Prompt 1:</strong></p> <blockquote> <p><em>“Calculate the mean velocity from this data.”</em></p> </blockquote> <p><strong>Prompt 2:</strong></p> <blockquote> <p><em>“Describe the trend shown in the velocity values.”</em></p> </blockquote> <p><strong>Prompt 3:</strong></p> <blockquote> <p><em>“Explain what this trend suggests about the motion.”</em></p> </blockquote> <hr/> <h2 id="moving-forward">Moving Forward</h2> <p>You now have a general understanding of how to prompt LLMs. We can then move on to how to construct prompts for data tasks, specifically.</p> <p>In the next module, we will apply these techniques to real science data analysis scenarios and explore how to design prompts that support authentic student sensemaking while maintaining scientific rigor.</p>]]></content><author><name>Emmanuel Adeloju</name></author><category term="LLMs"/><category term="prompt engineering"/><category term="data analysis"/><category term="science education"/><summary type="html"><![CDATA[A guide to choosing and applying the right prompting techniques for different levels of data tasks in science education.]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://prodholly.github.io/EmmanuelAdeloju/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://prodholly.github.io/EmmanuelAdeloju/blog/2025/plotly</id><content type="html" xml:base="https://prodholly.github.io/EmmanuelAdeloju/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry></feed>